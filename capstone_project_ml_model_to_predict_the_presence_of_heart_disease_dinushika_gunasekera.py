# -*- coding: utf-8 -*-
"""Capstone Project: ML Model to Predict the Presence of Heart Disease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Fr5J3QHm9YxJ37B9AJtGJ-CQVBeL9Mdl

# **Capstone Project: Machine Learning Foundation Course**
## **Machine Learning Model to Predict the Presence of Heart Disease**
#### Dataset: https://archive.ics.uci.edu/ml/datasets/Heart+Disease. 
#### Creators: Andras Janosi, M.D. (Hungarian Institute of Cardiology. Budapest), William Steinbrunn, M.D (University Hospital, Zurich, Switzerland), Matthias Pfisterer, M.D. (University Hospital, Basel, Switzerland), Robert Detrano, M.D., Ph.D. (V.A. Medical Center, Long Beach and Cleveland Clinic Foundation)

# **Student Information**
### Student Name: R.A.D. Dinushika Gunasekera
### Registration No: 255
### Date : 15-04-2022

## **Load Python Modules**
"""

#Importing the necessary libraries
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib import pyplot

# Data visualization
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import cm

# Classification algorithms
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB

#train test split
from sklearn.model_selection import train_test_split

# Model evaluation
from sklearn import metrics
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix 

# Parameter Serach Methods
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV

"""## **Load Data File**"""

file_path= 'https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data'

# Load CSV File
data = pd.read_csv(file_path)
data.head()

"""# **Assigning the Column Names**"""

with open('/content/sample_data/heart-disease.names') as f:
    print(f.read())

colum_headers = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']
data.columns = colum_headers
data.head(10)

# to create unique ID
data.index

data['ID'] = data.index
data.head()

data.columns

# to rearrange the columns
data = data[['ID', 'age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',
       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']]

data.head()

"""# **Data Pre-Processing**

## Analysing the Dataset
"""

# (rows, columns)
data.shape

# data types and non-null counts
data.info()

data.describe()

# analysing the actual values of the data variable to be predicted, 'num'
data['num'].unique()

# num = 0; disease absent
# num = 1, 2, 3, 4; disease present
# Hence introducing reslt = 0; disease absent and reslt  = 1; disease present

data['reslt'] = np.where(data['num']==0,0,1)

data[['num', 'reslt']].sample(10)

# to check whether balanced data counts are available. If not, need to undersample or oversample
data['reslt'].value_counts()

# to detect missing values (na)
data[data.isna().any(axis=1)]

# checking for null
data.isnull().sum()

# to detect missing values (?)
data.isin(['?']).value_counts()

# to obtain specific missing values related rows (ca - 4 rows, thal - 2 rows)
data[data.values == '?']

# analys the data values in column 'ca'
data['ca'].value_counts()

# analys the data values in column 'thal'
data['thal'].value_counts()

"""## Data Visualization"""

sns.set_style('darkgrid')
sns.set_palette('Set2')

data2 = data.copy()

def func1(sex):
    if sex == 0:
        return 'female'
    else:
        return 'male'
data2['sex'] = data2['sex'].apply(func1)

def func2(prob):
    if prob == 0:
        return 'No Heart Disease'
    else:
        return 'Heart Disease'
data2['reslt'] = data2['reslt'].apply(func2)

# count plots
sns.countplot(data= data2, x='sex',hue='reslt')
plt.title('Gender vs Result\n')

sns.countplot(data= data2, x='cp',hue='reslt')
plt.title('Chest Pain Type vs Result\n')

plt.figure(figsize=(16,7))
sns.distplot(data[data['reslt']==1]['age'],kde=False,bins=50)
plt.title('Age of Heart Diseased Patients\n')

plt.figure(figsize=(16,7))
sns.distplot(data[data['reslt']==1]['chol'],kde=False,bins=40)
plt.title('Cholesterol Level of Heart Diseased Patients\n')

"""## Treating Missing Values"""

# dropping the rows with missing values
data.drop(labels=[86, 165, 191, 265, 286, 301], axis=0, inplace=True)

# validating
data.isin(['?']).value_counts()

# checking the reslt counts after dropping the rows
data['reslt'].value_counts()

"""## Data Transformation"""

# no categorical data types to transform to numerical
# converting 'ca' and 'thal' datatypes to int64 due to the removal of '?'

data['ca'] = pd.to_numeric(data['ca'])
data['thal'] = pd.to_numeric(data['thal'])

data.dtypes

"""## Data Behaviors"""

data.hist(bins=50, figsize=(20, 20))

data.skew()

"""##### Skew is between -1 to 1. Anything more beyond this range is skewd. chol, fbs, oldpeak, ca, num are skewed

## Finding Outliers
"""

fig, ax = plt.subplots(2, 2, figsize=(25, 20))
data.boxplot(column=['age', 'sex', 'cp', 'trestbps'], ax=ax[0, 0])
data.boxplot(column=['chol', 'fbs', 'restecg', 'thalach' ], ax=ax[0, 1])
data.boxplot(column=['exang', 'oldpeak', 'slope'], ax=ax[1, 0])
data.boxplot(column=['ca', 'thal'], ax=ax[1, 1])

# to get individual plots
data['cp'].plot(kind='box')

"""#### Outliers are available in: cp, trestbps, chol, fbs, thalach, oldpeak and ca 


"""

# For cp

# 1st quartile
q1 = np.quantile(data['cp'], 0.25)
 
# 3rd quartile
q3 = np.quantile(data['cp'], 0.75)

iqr = q3-q1
 
# upper and lower whiskers
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)

outliers = data['cp'][(data['cp'] < lower_bound) | (data['cp']> upper_bound)]

print(F"IQR is: {iqr}")
print(F"Upper bound is: {upper_bound}")
print(F"Lower bound is: {lower_bound}")
print(F"The following are the outliers in the boxplot: \n {outliers}")

# For trestbps

# 1st quartile
q1 = np.quantile(data['trestbps'], 0.25)
 
# 3rd quartile
q3 = np.quantile(data['trestbps'], 0.75)

iqr = q3-q1
 
# upper and lower whiskers
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)

outliers = data['trestbps'][(data['trestbps'] < lower_bound) | (data['trestbps']> upper_bound)]

print(F"IQR is: {iqr}")
print(F"Upper bound is: {upper_bound}")
print(F"Lower bound is: {lower_bound}")
print(F"The following are the outliers in the boxplot: \n {outliers}")

# For chol

# 1st quartile
q1 = np.quantile(data['chol'], 0.25)
 
# 3rd quartile
q3 = np.quantile(data['chol'], 0.75)

iqr = q3-q1
 
# upper and lower whiskers
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)

outliers = data['chol'][(data['chol'] < lower_bound) | (data['chol']> upper_bound)]

print(F"IQR is: {iqr}")
print(F"Upper bound is: {upper_bound}")
print(F"Lower bound is: {lower_bound}")
print(F"The following are the outliers in the boxplot: \n {outliers}")

# For fbs

# 1st quartile
q1 = np.quantile(data['fbs'], 0.25)
 
# 3rd quartile
q3 = np.quantile(data['fbs'], 0.75)

iqr = q3-q1
 
# upper and lower whiskers
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)

outliers = data['fbs'][(data['fbs'] < lower_bound) | (data['fbs']> upper_bound)]

print(F"IQR is: {iqr}")
print(F"Upper bound is: {upper_bound}")
print(F"Lower bound is: {lower_bound}")
print(F"The following are the outliers in the boxplot: \n {outliers}")

# For thalach

# 1st quartile
q1 = np.quantile(data['thalach'], 0.25)
 
# 3rd quartile
q3 = np.quantile(data['thalach'], 0.75)

iqr = q3-q1
 
# upper and lower whiskers
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)

outliers = data['thalach'][(data['thalach'] < lower_bound) | (data['thalach']> upper_bound)]

print(F"IQR is: {iqr}")
print(F"Upper bound is: {upper_bound}")
print(F"Lower bound is: {lower_bound}")
print(F"The following are the outliers in the boxplot: \n {outliers}")

# For oldpeak

# 1st quartile
q1 = np.quantile(data['oldpeak'], 0.25)
 
# 3rd quartile
q3 = np.quantile(data['oldpeak'], 0.75)

iqr = q3-q1
 
# upper and lower whiskers
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)

outliers = data['oldpeak'][(data['oldpeak'] < lower_bound) | (data['oldpeak']> upper_bound)]

print(F"IQR is: {iqr}")
print(F"Upper bound is: {upper_bound}")
print(F"Lower bound is: {lower_bound}")
print(F"The following are the outliers in the boxplot: \n {outliers}")

# For ca

# 1st quartile
q1 = np.quantile(data['ca'], 0.25)
 
# 3rd quartile
q3 = np.quantile(data['ca'], 0.75)

iqr = q3-q1
 
# upper and lower whiskers
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)

outliers = data['ca'][(data['ca'] < lower_bound) | (data['ca']> upper_bound)]

print(F"IQR is: {iqr}")
print(F"Upper bound is: {upper_bound}")
print(F"Lower bound is: {lower_bound}")
print(F"The following are the outliers in the boxplot: \n {outliers}")

"""## Treating Outliers

### Method: Droping the rows
"""

# cp 
data.drop([19, 20, 26, 29, 40, 58, 100, 111, 123, 140, 149, 181, 182, 195, 198, 210, 214, 242, 261, 273, 274, 297], axis=0, inplace=True)

data['cp'].plot(kind='box')

# trestbps
data.drop([13, 82, 125, 171, 187, 200, 212, 230], axis=0, inplace=True)

data['trestbps'].plot(kind='box')

# chol
data.drop([47, 120, 151, 172, 180], axis=0, inplace=True)

data['chol'].plot(kind='box')

# thalach
data.drop([244], axis=0, inplace=True)

data['thalach'].plot(kind='box')

# oldpeak
data.drop([90, 122, 190, 284], axis=0, inplace=True)

data['oldpeak'].plot(kind='box')

# ca
data.drop([0, 39, 61, 91, 103, 117, 145, 154, 160, 175, 178, 186, 188, 192, 204, 231], axis=0, inplace=True)

data['ca'].plot(kind='box')

# rechecking for outliers

fig, ax = plt.subplots(2, 2, figsize=(25, 20))
data.boxplot(column=['age', 'sex', 'cp', 'trestbps'], ax=ax[0, 0])
data.boxplot(column=['chol', 'fbs', 'restecg', 'thalach' ], ax=ax[0, 1])
data.boxplot(column=['exang', 'oldpeak', 'slope'], ax=ax[1, 0])
data.boxplot(column=['ca', 'thal'], ax=ax[1, 1])

# For thalach

# 1st quartile
q1 = np.quantile(data['thalach'], 0.25)
 
# 3rd quartile
q3 = np.quantile(data['thalach'], 0.75)

iqr = q3-q1
 
# upper and lower whiskers
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)

outliers = data['thalach'][(data['thalach'] < lower_bound) | (data['thalach']> upper_bound)]

print(F"IQR is: {iqr}")
print(F"Upper bound is: {upper_bound}")
print(F"Lower bound is: {lower_bound}")
print(F"The following are the outliers in the boxplot: \n {outliers}")

data.drop([174, 295], axis=0, inplace=True)

# For oldpeak

# 1st quartile
q1 = np.quantile(data['oldpeak'], 0.25)
 
# 3rd quartile
q3 = np.quantile(data['oldpeak'], 0.75)

iqr = q3-q1
 
# upper and lower whiskers
upper_bound = q3+(1.5*iqr)
lower_bound = q1-(1.5*iqr)

outliers = data['oldpeak'][(data['oldpeak'] < lower_bound) | (data['oldpeak']> upper_bound)]

print(F"IQR is: {iqr}")
print(F"Upper bound is: {upper_bound}")
print(F"Lower bound is: {lower_bound}")
print(F"The following are the outliers in the boxplot: \n {outliers}")

data.drop([292], axis=0, inplace=True)

fig, ax = plt.subplots(2, 2, figsize=(25, 20))
data.boxplot(column=['age', 'sex', 'cp', 'trestbps'], ax=ax[0, 0])
data.boxplot(column=['chol', 'fbs', 'restecg', 'thalach' ], ax=ax[0, 1])
data.boxplot(column=['exang', 'oldpeak', 'slope'], ax=ax[1, 0])
data.boxplot(column=['ca', 'thal'], ax=ax[1, 1])

# checking the restl counts after dropping the rows
data['reslt'].value_counts()

"""#### As the reslt value counts are fairly balanced, need not to versample or undersample"""

data.shape

"""## Checking the Correlation"""

correlation_matrix = data.corr()

plt.figure(figsize = (15,8))
sns.heatmap(correlation_matrix, annot=True)
plt.show()
correlation_matrix

Selected = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'reslt' ]

sns.pairplot(data=data[Selected], hue='reslt', markers='+')
plt.show()

"""## Feature Selection"""

# selecting x and y variables
X_variables = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']
y_variable = 'reslt'

print(F"X_variables = {X_variables}")
print(F"y_variable = {y_variable}")

X = data[X_variables].values
y = data[y_variable].values

"""## Test Train Split"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

print(F"Train sample size = {len(X_train)}")
print(F"Test sample size  = {len(X_test)}")

"""# **Model Building**"""

# function to train the model

def model_train(model, model_name, X_train, y_train, X_test, y_test):
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    y_pred_prob = model.predict_proba(X_test)[:, 1]
    test_result = pd.DataFrame(data={'y_act':y_test, 'y_pred':y_pred, 'y_pred_prob':y_pred_prob})

    accuracy = metrics.accuracy_score(test_result['y_act'], test_result['y_pred']) 
    precision = metrics.precision_score(test_result['y_act'], test_result['y_pred'], average='binary', pos_label=1)
    recall = metrics.recall_score(test_result['y_act'], test_result['y_pred'], average='weighted')
    f1_score = metrics.f1_score(test_result['y_act'], test_result['y_pred'], average='weighted')  
    roc_auc = metrics.roc_auc_score(test_result['y_act'], test_result['y_pred_prob'])

    

    return ({'model_name':model_name, 
                   'model':model, 
                   'accuracy':accuracy, 
                   'precision':precision,
                  'recall':recall,
                  'f1_score':f1_score,
                  'roc_auc':roc_auc,
                  'y act': y_test,
                  'y pred': y_pred
                  })

# model evaluation

models = []
models.append(model_train(LogisticRegression(n_jobs=3, random_state = 0), 'LGR', X_train, y_train, X_test, y_test))
models.append(model_train(KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2), 'KNC', X_train, y_train, X_test, y_test))
models.append(model_train(RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0), 'RF', X_train, y_train, X_test, y_test))
models.append(model_train(DecisionTreeClassifier(criterion = 'gini', random_state = 0 ), 'DT-G', X_train, y_train, X_test, y_test))
models.append(model_train(DecisionTreeClassifier(criterion = 'entropy', random_state = 0 ), 'DT-En', X_train, y_train, X_test, y_test))
models = pd.DataFrame(models)
models

"""## Individual Model Building

### Logistic Regression
"""

# Create model object
model = LogisticRegression()

# Train Model
model.fit(X_train, y_train)

# Predict Probability -use the test data set
model.predict_proba(X_test)[:5]

# Predict on Testing Data

# probabilities
y_pred_prob = model.predict_proba(X_test)[:, 1]

# Prediction
y_pred = model.predict(X_test)

# Results table
test_result = pd.DataFrame(data={'y_act':y_test, 'y_pred':y_pred, 'y_pred_prob':y_pred_prob})
test_result.sample(5)

cfm = pd.crosstab(test_result['y_act'], test_result['y_pred'], margins=True)
cfm

# Accuracy
accuracy_lgr = metrics.accuracy_score(test_result['y_act'], test_result['y_pred']) 
print(F"accuracy_lgr = {accuracy_lgr}")

# Precision
precision_lgr = metrics.precision_score(test_result['y_act'], test_result['y_pred'], average='binary', pos_label=1)
print(F"Precision_lgr = {precision_lgr}")

# Recall
recall_lgr = metrics.recall_score(test_result['y_act'], test_result['y_pred'], average='weighted') #weighted accounts for label imbalance.
print(F"Recall_lgr = {recall_lgr}")

# F1 score
f1_score_lgr = metrics.f1_score(test_result['y_act'], test_result['y_pred'], average='weighted')  #weighted accounts for label imbalance.
print(F"F1 score_lgr = {f1_score_lgr}")

# classification report - this gives the full report of the performance 
from sklearn.metrics import classification_report  
print("classification_report:\n",classification_report(test_result['y_act'], test_result['y_pred']))

# Feature importance/Coefficients
features_to_model = X_variables
coefficients = model.coef_[0]
intercept = model.intercept_[0]
feature_profile = pd.DataFrame({"feature":features_to_model, "coefficients":coefficients})
print("feature_profile:\n", feature_profile)
print("intercept:", intercept)
print('\n')
print("Model Parameters:\n", pd.Series(model.get_params()))

"""## **Parameter Tunning**

### GridSearchCV
"""

# Define Hyperparameter Grid
param_grid = {'C': [0.1, 0.5, 1, 2, 5, 10]} # only one parameter is given here which is C. also can use functions range or np arrange to define the range
  
# Create model object
model = LogisticRegression()
  
# Create GridSearchCV object
model_cv = GridSearchCV(model, param_grid, cv=5, scoring='f1')  # important: scoring - based on which parameter evaluation should be done
  
model_cv.fit(X_train, y_train)
  
# Print the tuned parameters and score
print("Tuned Model Parameters: {}".format(model_cv.best_params_))
print("Best model score: {}".format(model_cv.best_score_))

"""Tuned Model Parameters: {'C': 0.5}

Best model score: 0.8003589743589743

### Get Best Model
"""

model = model_cv.best_estimator_

# Feature importance/Coefficients
coefficients = model.coef_[0]
intercept = model.intercept_[0]
feature_profile = pd.DataFrame({"feature":features_to_model, "coefficients":coefficients})
print("feature_profile:\n", feature_profile)
print("intercept:", intercept)
print('\n')
print("Model Parameters:\n", pd.Series(model.get_params()))

# Evaluate Model
y_pred = model.predict(X_test)
y_pred_prob = model.predict_proba(X_test)[:,1]
print(y_pred_prob[:5])

test_result = pd.DataFrame(data={'y_act':y_test, 'y_pred':y_pred, 'y_pred_prob':y_pred_prob})
print(test_result.sample(10))
print('\n')


cfm = pd.crosstab(test_result['y_act'], test_result['y_pred'], margins=True)
print("Confusion Matrix:\n", cfm)
print('\n')

# Model evaluation
# Accuracy
accuracy_lgr = metrics.accuracy_score(test_result['y_act'], test_result['y_pred']) 
print(F"accuracy_lgr = {accuracy_lgr}")

# Precision
precision_lgr = metrics.precision_score(test_result['y_act'], test_result['y_pred'], average='binary', pos_label=1)
print(F"Precision_lgr = {precision_lgr}")

# Recall
recall_lgr = metrics.recall_score(test_result['y_act'], test_result['y_pred'], average=None) #weighted accounts for label imbalance.
print(F"Recall_lgr = {recall_lgr}")

# F1 score
f1_score_lgr = metrics.f1_score(test_result['y_act'], test_result['y_pred'], average=None)  #weighted accounts for label imbalance.
print(F"F1 score_lgr = {f1_score_lgr}")

from sklearn.metrics import classification_report
print("classification_report:\n",classification_report(test_result['y_act'], test_result['y_pred']))

# Select best model 
model = models.query("model_name=='LGR'")
model

#extracting the model name from the dictonery 
model = model['model'].values[0]
model

"""# **Saving the Model**"""

import pickle

save_file = 'model_LGR.pickle'
pickle.dump(model, open(save_file, 'wb'))

# loading from file
model_ = pickle.load(open(save_file, 'rb'))
model_